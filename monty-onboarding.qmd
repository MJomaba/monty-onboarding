---
title: "monty onboarding"
author: "Marc Baguelin"
format: revealjs
editor: visual
---

# Background

## dust and mcstate

```{r setup}
library(monty)
```

-   *dust* & *mcstate* triggered by epi + software team discussions
-   Motivated by real-time fitting of stochastic model
-   *dust* deals with efficient parallel computation
-   *mcstate* was an inference-for-odin-model package

## More on mcstate

-   Developed during the pandemic (technical debt)
-   Focus on Sequential Monte Carlo (SMC aka particle filter)
-   Workhorse algorithm = pMCMC with bootstrap

## Beyond the "discrete-stochastic-odin"

-   More ideas have emerged since
    -   adaptive algorithm
    -   nested hierarchical fitting
    -   HMC
    -   parallel tempering
    -   hybrid time schedule
-   Development hindered by *mcstate* architecture

# monty general principles

## What do we call models?

:::::: columns
::: {.column width="45%"}
**SimCity models**

Input (parameters) delivers real-life outputs

![](images/sim_city_models.webp){fig-align="left" style="position: absolute; bottom: 0; left: 0;" width="30%"}
:::

::: {.column width="10%"}
<!-- empty column to create gap -->

**vs**
:::

::: {.column width="45%"}
**Statistical models**

Abstract (parameter) space with a density function

!["An image of a simple statistical density model but in high dimension"](images/density_model.webp){fig-align="left" style="position: absolute; bottom: 0; left: 1200;" width="40%"}
:::
::::::

## Linking the two

- odin deals with SimCity models
- inference algorithms deal with statistical model
- Special functions were created in *mcstate* to embed the *odin* model into a statistical model
- the compare function matches the states of the SimCity model with data to produce a density
- the transform functions transform simple (*mcstate*) parameter object into a list of input for the (*odin*) SimCity model

## Issues

- No "statistical model" object in mcstate - closest being the "particle filter"
- Restricted to particle filter family
- Somehow complicated to think about abstract parameters + transform fun + odin code + data + compare fun + sampler
- Creates inefficiencies does not exploit that stat parameters are simpler
- Difficult to separate the modelling from the inference

## Design solution

- Compare function is an "observation" model -> moves to odin
- Model + data produces a likelihood that is a statistical model, voilà
- transform was doing two functions:
  - any transformation of parameters part of model in *odin*
  - packer function (not a model) put the parameters in shape for stat or SimCity model, and tells what are fixed i.e. not statistical parameters

## A more modular approach

![odin playing with duplo bricks](images/odin-monty-duplo.webp){#fig-odin-duplo fig-alt="ALT" width="30%"}

## Summary

## Tutorial

## The Basic Idea

Draw samples from a model using Markov Chain Monte Carlo methods. To do this you will need:

1.  A **model**:
    -   A `monty_model()` object
    -   Computes a log probability density
    -   May compute gradient of the log density or sample directly from parameter space.
2.  A **sampler**:
    -   A method of drawing samples from the model’s distribution.
    -   Simplest sampler is `monty_sampler_random_walk()`, which implements a simple Metropolis algorithm.
3.  A **runner**:
    -   Controls how chains are run (e.g., in sequence or in parallel).

## Composability

-   The system is **composable**:
    -   Define a model for likelihood
    -   Define another model for prior
    -   Choose a sampler and runner.
-   `monty_model()` is flexible but not user-friendly. A high-level interface will help wrap models from other packages (e.g., [dust](https://mrc-ide.github.io/dust), [odin](https://mrc-ide.github.io/odin)).

## An Example

> Tools like **stan**, **bugs**, **jags** are generally better. This example shows a simple model to demonstrate the approach in **monty**.

```{r, echo=TRUE}
set.seed(1)
data <- local({
  n <- 60
  weight <- rnorm(n, 50, 6.5)
  height <- 114 + weight * 0.9 + rnorm(60, sd = 3)
  data.frame(height, weight)
})
```

```{r, echo=TRUE}
head(data)
plot(height ~ weight, data)
```

## Simple Likelihood

A simple likelihood modelled as normally distributed departures from a linear relationship with weight.

```{r, echo=TRUE}
fn <- function(a, b, sigma, data) {
  mu <- a + b * data$weight
  sum(dnorm(data$height, mu, sigma, log = TRUE))
}
```

## Wrapping Likelihood

Wrap the density function in a `monty_model`. The `data` argument is “fixed” and passed as the `fixed` argument.

```{r, echo=TRUE}
likelihood <- monty_model_function(fn, fixed = list(data = data))
likelihood
```

## Defining a Prior

Priors are defined using the `monty_dsl`:

```{r, echo=TRUE}
prior <- monty_dsl({
  a ~ Normal(178, 100)
  b ~ Normal(0, 10)
  sigma ~ Uniform(0, 50)
})
prior
```

## Posterior Distribution

Combine the likelihood and prior into a posterior distribution. This can be done using the `+` operator.

```{r, echo=TRUE}
posterior <- likelihood + prior
posterior
```

## Sampler Setup

Construct a sensible initial variance-covariance matrix and define a random walk sampler.

```{r, echo=TRUE}
vcv <- rbind(c(4.5, -0.088, 0.076),
             c(-0.088, 0.0018, -0.0015),
             c(0.076, -0.0015, 0.0640))
sampler <- monty_sampler_random_walk(vcv = vcv)
```

## Running the Sampler

Run the sampler to draw 2000 samples across 4 chains.

```{r, echo=TRUE}
samples <- monty_sample(posterior, sampler, 2000, initial = c(114, 0.9, 3),
                        n_chains = 4)
```

## Visualisation

You can plot the log posterior density over time:

```{r, echo=TRUE}
matplot(samples$density, type = "l", lty = 1,
        xlab = "log posterior density", ylab = "sample", col = "#00000055")
```

## Parameter Estimation

Estimate the posterior densities of parameters `a`, `b`, and `sigma`.

```{r, echo=TRUE}
par(mfrow = c(1, 3))
plot(density(samples$pars["a", , ]), main = "a")
abline(v = 114, col = "red")
plot(density(samples$pars["b", , ]), main = "b")
abline(v = 0.9, col = "red")
plot(density(samples$pars["sigma", , ]), main = "sigma")
abline(v = 3, col = "red")
```


## Setting the Scene

We'll start with a simple dataset of daily cases of some disease over time.

```r
data <- read.csv("incidence.csv")
head(data)
plot(cases ~ time, data, pch = 19, las = 1,
     xlab = "Time (days)", ylab = "Cases")
```

The data here shows a classic epidemic, with cases rising up to some peak and falling.

---

## SIR Model with odin2

We will try fitting this with a simple compartmental **SIR model** using `odin2`. We'll start with a stochastic discrete-time version.

```r
sir <- odin2::odin({
  initial(S) <- N - I0
  initial(I) <- I0
  initial(R) <- 0
  initial(incidence) <- 0
  update(S) <- S - n_SI
  update(I) <- I + n_SI - n_IR
  update(R) <- R + n_IR
  update(incidence) <- if (time %% 1 == 0) n_SI else incidence + n_SI
  n_SI <- Binomial(S, p_SI)
  n_IR <- Binomial(I, p_IR)
  p_SI <- 1 - exp(-beta * I / N * dt)
  p_IR <- 1 - exp(-gamma * dt)
  beta <- parameter()
  gamma <- parameter()
  I0 <- parameter()
  N <- 1000
}, quiet = TRUE)
```

---

## Simulation and Plotting

We can now simulate the model over the time series and plot the results.

```r
pars <- list(beta = 0.3, gamma = 0.1, I0 = 5)
sys <- dust2::dust_system_create(sir(), pars, n_particles = 20, dt = 0.25)
dust2::dust_system_set_state_initial(sys)
time <- 0:100
y <- dust2::dust_system_simulate(sys, time)
```

```r
matplot(time, t(y[4, , ]), type = "l", lty = 1, col = "#00000055",
        xlab = "Time (days)", ylab = "Cases", las = 1)
points(cases ~ time, data, pch = 19, col = "red")
```

---

## Comparing to Data

Now, we will compare the simulation to data using a **Poisson likelihood**. This asks, "What is the probability of observing this many cases with a mean equal to our modelled number of daily cases?"

```r
sir <- odin2::odin({
  ...
  cases <- data()
  cases ~ Poisson(incidence)
}, quiet = TRUE)
```

---

## Particle Filter and Likelihood

We will now use a **particle filter** to estimate the marginal likelihood by averaging over stochasticity.

```r
filter <- dust2::dust_filter_create(sir(), 0, data, n_particles = 200)
dust2::dust_likelihood_run(filter, pars)
```

Run this multiple times to see the variance in likelihood:

```r
ll1 <- replicate(100, dust2::dust_likelihood_run(filter, pars1))
ll2 <- replicate(100, dust2::dust_likelihood_run(filter, pars2))
```

---

## Posterior with monty

We combine the likelihood with a **prior** to create a **posterior**. We'll sample from the posterior using MCMC.

```r
prior <- monty::monty_dsl({
  beta ~ Exponential(mean = 0.3)
  gamma ~ Exponential(mean = 0.1)
})
posterior <- prior + likelihood
sampler <- monty::monty_sampler_random_walk(diag(2) * 0.02)
samples <- monty::monty_sample(posterior, sampler, 100, initial = sir_packer$pack(pars))
```

---

## MCMC Results

Finally, we examine the MCMC results:

```r
plot(samples$density, type = "l")
plot(t(drop(samples$pars)), pch = 19, col = "#00000055")
```

---

## Improving the Model

To improve the model mixing, we will:

- Use a better proposal kernel.
- Increase the number of particles.
- Run multiple chains in parallel.
  
```r
vcv <- matrix(c(0.0005, 0.0003, 0.0003, 0.0003), 2, 2)
filter <- dust2::dust_unfilter_create(sir(), 0, data, n_particles = 1000)
samples <- monty::monty_sample(posterior, sampler, 1000, initial = sir_packer$pack(pars))
```

```r
plot(samples$density, type = "l")
plot(t(drop(samples$pars)), pch = 19, col = "#00000055")
```

---

## Conclusion

We explored fitting an SIR model using `odin2`, `dust2`, and `monty`, compared it to data, and applied a particle filter to compute likelihoods. We also discussed MCMC sampling to explore parameter space.


## Further Tools

Convert the samples for use with other tools:

-   `coda::as.mcmc.list()` for MCMC
-   `posterior::as_draws_df()` for further analysis

See `vignette("samplers")` for more details.

# Tidying up odin and dust
